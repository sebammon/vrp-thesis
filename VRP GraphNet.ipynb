{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "255b204d",
   "metadata": {},
   "source": [
    "# VRP GraphNet\n",
    "\n",
    "model inputs from the paper:\n",
    "\n",
    "| Variable             | Meaning                           | Dimensions                |\n",
    "|----------------------|-----------------------------------|---------------------------|\n",
    "| batch_edges          | Adj matrix special connections*   | B x num_nodes x num_nodes |\n",
    "| batch_edges_values   | Distance Matrix                   | B x num_nodes x num_nodes |\n",
    "| batch_edges_target   | Target adj matrix                 | B x num_nodes x num_nodes |\n",
    "| batch_nodes          | Ones vector                       | B x num_nodes             |\n",
    "| batch_nodes_coord    | Coordinates                       | B x num_nodes x 2         |\n",
    "| *batch_nodes_target* | Value represents ordering in tour | B x num_nodes             |\n",
    "\n",
    "\n",
    "*special connections:\n",
    "* 1 - k-nearest neighbour\n",
    "* 2 - self connections\n",
    "* 0 - otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/gdrive')\n",
    "\n",
    "    %cd gdrive/My Drive/machine-learning-course\n",
    "    %pip install -r requirements-colab.txt\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir runs\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import utils.beam_search as beam_search\n",
    "from model import GraphNet\n",
    "from utils import load_config, get_metrics, get_device, save_checkpoint, _n\n",
    "from utils.data import load_and_split_dataset, process_datasets, sparse_matrix_from_routes, distance_from_sparse_matrix\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b562d",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7a9efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = load_and_split_dataset(\"data/vrp_20_3s_random_depot.pkl\", test_size=1000)\n",
    "train_dataset, test_dataset = process_datasets(dsets, k=6)\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "edge_class_weights = train_dataset.class_weights()\n",
    "edge_class_weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "b7aed5e1",
   "metadata": {},
   "source": [
    "## Basic Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4347cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e02b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = load_config(hidden_dim=32, num_gcn_layers=5, num_mlp_layers=3,\n",
    "                             learning_rate=0.001, train_batch_size=64, test_batch_size=64, num_epochs=50)\n",
    "default_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27706ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=default_config.train_batch_size,\n",
    "                              shuffle=True)\n",
    "\n",
    "model = GraphNet(default_config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca258e29",
   "metadata": {},
   "source": [
    "## Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b13064",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, _ = next(iter(train_dataloader))\n",
    "\n",
    "y_pred = model.forward(features[\"node_features\"].to(device),\n",
    "                       features[\"dist_matrix\"].to(device),\n",
    "                       features[\"edge_feat_matrix\"].to(device))\n",
    "\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29093f8",
   "metadata": {},
   "source": [
    "## Validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1009ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_violations(tours, batch_node_features):\n",
    "    violations = []\n",
    "\n",
    "    for i, tour in enumerate(tours):\n",
    "        violations_count = 0\n",
    "        capacity = 0\n",
    "        for j in range(len(tour) + 1):\n",
    "            node = tour[j] if j < len(tour) else 0\n",
    "\n",
    "            if node == 0:\n",
    "                if capacity > 1:\n",
    "                    violations_count += 1\n",
    "                capacity = 0\n",
    "            else:\n",
    "                capacity += batch_node_features[i, node, 2]\n",
    "\n",
    "        violations.append(violations_count)\n",
    "\n",
    "    return np.array(violations)\n",
    "\n",
    "\n",
    "def greedy_tour_lengths(y_preds, batch_dist_matrix, num_vehicles):\n",
    "    # only keep the probability of selecting the edge\n",
    "    y_preds = y_preds[..., 1]\n",
    "\n",
    "    beamsearch = beam_search.BeamSearch(y_preds, beam_width=1, num_vehicles=num_vehicles)\n",
    "    beamsearch.search()\n",
    "\n",
    "    # get most probable tours (index = 0)\n",
    "    tours = beamsearch.get_beam(0)\n",
    "\n",
    "    sp_targets = sparse_matrix_from_routes(tours, batch_dist_matrix.size(-1))\n",
    "    tour_lengths = distance_from_sparse_matrix(sp_targets, batch_dist_matrix)\n",
    "\n",
    "    return tour_lengths, tours\n",
    "\n",
    "\n",
    "def shortest_tour_lengths(y_preds, batch_dist_matrix, num_vehicles, beam_width=1024):\n",
    "    # only keep the probability of selecting the edge\n",
    "    y_preds = y_preds[..., 1]\n",
    "\n",
    "    beamsearch = beam_search.BeamSearch(y_preds, beam_width=beam_width, num_vehicles=num_vehicles)\n",
    "    beamsearch.search()\n",
    "\n",
    "    shortest_tours = torch.zeros((batch_dist_matrix.size(0), len(beamsearch.next_nodes)))\n",
    "    shortest_tour_distances = torch.full((batch_dist_matrix.size(0),), np.inf)\n",
    "\n",
    "    for i in range(beamsearch.beam_width):\n",
    "        tours = beamsearch.get_beam(i)\n",
    "\n",
    "        # creating the sparse matrix is expensive\n",
    "        sp_adj_matrix = sparse_matrix_from_routes(tours, batch_dist_matrix.size(-1))\n",
    "        tour_lengths = distance_from_sparse_matrix(sp_adj_matrix, batch_dist_matrix)\n",
    "\n",
    "        # keep the shortest tours\n",
    "        condition = torch.lt(tour_lengths, shortest_tour_distances).unsqueeze(-1)\n",
    "        shortest_tours = torch.where(condition, tours, shortest_tours)\n",
    "\n",
    "        # keep the shortest tour lengths\n",
    "        shortest_tour_distances = torch.minimum(shortest_tour_distances, tour_lengths)\n",
    "\n",
    "    return shortest_tour_distances, shortest_tours\n",
    "\n",
    "\n",
    "def eval_model(batch_node_features, batch_dist_matrix, batch_edge_features, model):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(batch_node_features, batch_dist_matrix, batch_edge_features)\n",
    "        preds = F.softmax(preds, dim=3)\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "def validate(dataloader, model, criterion):\n",
    "    running_loss = 0\n",
    "    running_tour_lengths = []\n",
    "    running_tour_violations = []\n",
    "    targets = []\n",
    "    predictions = []\n",
    "\n",
    "    for batch_features, batch_targets in dataloader:\n",
    "        batch_node_features = batch_features[\"node_features\"].to(device)\n",
    "        batch_dist_matrix = batch_features[\"dist_matrix\"].to(device)\n",
    "        batch_edge_features = batch_features[\"edge_feat_matrix\"].to(device)\n",
    "        batch_num_vehicles = batch_features[\"num_vehicles\"].to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        y_preds = eval_model(batch_node_features, batch_dist_matrix, batch_edge_features,\n",
    "                             model=model)\n",
    "\n",
    "        # Loss\n",
    "        loss = get_loss(y_preds, batch_targets, criterion)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Tour lengths (mean per batch)\n",
    "        for vehicles in torch.unique(batch_num_vehicles):\n",
    "            mask = torch.eq(batch_num_vehicles, vehicles)\n",
    "\n",
    "            tour_lengths, tours = greedy_tour_lengths(y_preds[mask].cpu(), batch_dist_matrix[mask].cpu(),\n",
    "                                                      num_vehicles=vehicles)\n",
    "\n",
    "            running_tour_violations.extend(count_violations(tours, batch_node_features[mask]))\n",
    "            running_tour_lengths.extend(_n(tour_lengths))\n",
    "\n",
    "        y_preds = y_preds.argmax(dim=3)\n",
    "        y_preds = y_preds.cpu().numpy()\n",
    "\n",
    "        targets.append(batch_targets.cpu().numpy())\n",
    "        predictions.append(y_preds)\n",
    "\n",
    "    targets = np.concatenate(targets)\n",
    "    predictions = np.concatenate(predictions)\n",
    "    mean_running_loss = running_loss / len(dataloader)\n",
    "\n",
    "    running_tour_lengths = np.mean(running_tour_lengths)\n",
    "    running_tour_violations = np.mean(running_tour_violations)\n",
    "\n",
    "    return targets, predictions, mean_running_loss, running_tour_lengths, running_tour_violations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e3f97",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e0daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(preds, targets, criterion):\n",
    "    preds_perm = preds.permute(0, 3, 1, 2)\n",
    "\n",
    "    return criterion(preds_perm, targets)\n",
    "\n",
    "\n",
    "def train_one_epoch(dataloader, model, optimizer, criterion):\n",
    "    running_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (batch_features, batch_targets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_node_features = batch_features[\"node_features\"].to(device)\n",
    "        batch_dist_matrix = batch_features[\"dist_matrix\"].to(device)\n",
    "        batch_edge_features = batch_features[\"edge_feat_matrix\"].to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        preds = model(batch_node_features, batch_dist_matrix, batch_edge_features)\n",
    "        loss = get_loss(preds, batch_targets, criterion)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss\n",
    "\n",
    "\n",
    "def train(num_epochs, train_dl, test_dl, model, optimizer, criterion, writer):\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        running_loss = train_one_epoch(train_dl, model=model, optimizer=optimizer, criterion=criterion)\n",
    "\n",
    "        # Losses\n",
    "        epoch_loss = running_loss / len(train_dl)\n",
    "\n",
    "        # Validation Metrics\n",
    "        targets, predictions, validation_loss, tour_length, violations = validate(test_dl, model=model,\n",
    "                                                                                  criterion=criterion)\n",
    "        metrics = get_metrics(targets, predictions)\n",
    "\n",
    "        writer.add_scalar(\"Metrics/accuracy\", metrics.acc, epoch)\n",
    "        writer.add_scalar(\"Metrics/bal. accuracy\", metrics.bal_acc, epoch)\n",
    "        writer.add_scalar(\"Metrics/precision\", metrics.precision, epoch)\n",
    "        writer.add_scalar(\"Metrics/recall\", metrics.recall, epoch)\n",
    "        writer.add_scalar(\"Metrics/f1 score\", metrics.f1_score, epoch)\n",
    "        writer.add_scalar(\"Metrics/tour length\", tour_length, epoch)\n",
    "        writer.add_scalar(\"Metrics/violations\", violations, epoch)\n",
    "\n",
    "        writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/test\", validation_loss, epoch)\n",
    "\n",
    "        # Save (validation) checkpoint\n",
    "        if validation_loss < best_loss:\n",
    "            best_loss = validation_loss\n",
    "            save_checkpoint(writer.log_dir / \"best_validation_loss_model.pt\",\n",
    "                            model=model, optimizer=optimizer,\n",
    "                            epoch=epoch, config={**config}, train_loss=epoch_loss, test_loss=validation_loss)\n",
    "\n",
    "        # Save (epoch) checkpoint\n",
    "        save_checkpoint(writer.log_dir / \"last_epoch_model.pt\",\n",
    "                        model=model, optimizer=optimizer,\n",
    "                        epoch=epoch, config={**config}, train_loss=epoch_loss, test_loss=validation_loss)\n",
    "\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483b5b2d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LOG_DIR = Path(f\"runs/exp_baseline\")\n",
    "\n",
    "config = load_config(**default_config)\n",
    "config.hidden_dim = 16\n",
    "config.num_gcn_layers = 5\n",
    "config.num_epochs = 20\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.train_batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=config.test_batch_size, shuffle=True)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "edge_class_weights = train_dataset.class_weights().to(device)\n",
    "model = GraphNet(config).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(edge_class_weights)\n",
    "\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "\n",
    "train(config.num_epochs, train_dl=train_dataloader, test_dl=test_dataloader,\n",
    "      model=model, optimizer=optimizer, criterion=criterion, writer=writer)\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random instances to plot\n",
    "batch_features, batch_targets = next(iter(test_dataloader))\n",
    "\n",
    "num_plots = 10\n",
    "\n",
    "choices = np.random.choice(len(batch_targets), num_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.plot import plot_graph, plot_heatmap, plot_beam_search_tour\n",
    "from utils.data import distance_from_adj_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_node_features = batch_features[\"node_features\"].to(device)\n",
    "batch_dist_matrix = batch_features[\"dist_matrix\"].to(device)\n",
    "batch_edge_features = batch_features[\"edge_feat_matrix\"].to(device)\n",
    "batch_num_vehicles = batch_features[\"num_vehicles\"].to(device)\n",
    "batch_targets = batch_targets.to(device)\n",
    "\n",
    "preds = eval_model(batch_node_features, batch_dist_matrix, batch_edge_features,\n",
    "                   model=model)\n",
    "\n",
    "ground_truth_distance = distance_from_adj_matrix(batch_targets, batch_dist_matrix)\n",
    "shortest_distance, tours = shortest_tour_lengths(preds.cpu(), batch_dist_matrix.cpu(), beam_width=1024,\n",
    "                                                 num_vehicles=torch.max(batch_num_vehicles))\n",
    "\n",
    "for i in choices:\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    plot_graph(_n(batch_node_features[i, :, :2]), _n(batch_targets[i]), ax=ax[0])\n",
    "    plot_heatmap(_n(batch_node_features[i, :, :2]), _n(batch_targets[i]), _n(preds[i][..., 1]), ax=ax[1])\n",
    "    plot_beam_search_tour(_n(batch_node_features[i, :, :2]), _n(batch_targets[i]), _n(tours[i]), ax=ax[2])\n",
    "\n",
    "    ax[0].set_title(f\"Ground truth ({ground_truth_distance[i]:.2f})\")\n",
    "    ax[1].set_title(\"Predictions\")\n",
    "    ax[2].set_title(f\"Shortest tour ({shortest_distance[i]:.2f})\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
