{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c22e3c3",
   "metadata": {},
   "source": [
    "## Paper: Efficiently solving the practical vehicle routing problem\n",
    "\n",
    "Source: `Duan et al., ‘Efficiently Solving the Practical Vehicle Routing Problem’.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "ed8492b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "38bd6802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "5583a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTANCE_PATH = Path(\"data\")\n",
    "RESULTS_PATH = Path(\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "dd5f18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INSTANCE_PATH / 'dataset.pkl', 'rb') as f:\n",
    "    graphs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "4f012187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.5       , 0.5       , 0.        ],\n",
       "        [0.06505159, 0.94888554, 2.        ],\n",
       "        [0.96563203, 0.80839735, 4.        ],\n",
       "        [0.30461377, 0.09767211, 8.        ],\n",
       "        [0.68423303, 0.44015249, 7.        ],\n",
       "        [0.12203823, 0.49517691, 9.        ],\n",
       "        [0.03438852, 0.9093204 , 8.        ],\n",
       "        [0.25877998, 0.66252228, 5.        ],\n",
       "        [0.31171108, 0.52006802, 2.        ],\n",
       "        [0.54671028, 0.18485446, 5.        ],\n",
       "        [0.96958463, 0.77513282, 8.        ]]),\n",
       " array([-1,  0,  1,  2,  1,  0,  0,  0,  2,  2,  1]))"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "4bbdb023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(pointA, pointB):\n",
    "    # Efficient way to calculate the euclidean distance\n",
    "    return np.linalg.norm(pointA - pointB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "c1508571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_graph(graph):\n",
    "    nodes, label = graph\n",
    "    \n",
    "    label = torch.tensor(label + 1, dtype=torch.int64)\n",
    "    graph = torch.tensor(nodes[:, :-1], dtype=torch.float32)\n",
    "    demand = torch.tensor(nodes[:, -1], dtype=torch.float32)\n",
    "    distance = np.zeros((nodes.shape[0], nodes.shape[0]))\n",
    "\n",
    "    for i in range(len(distance)):\n",
    "        for j in range(i + 1, len(distance)):\n",
    "            d = get_distance(graph[i], graph[j])\n",
    "            distance[i][j] = d\n",
    "            distance[j][i] = d\n",
    "            \n",
    "    distance = torch.tensor(distance, dtype=torch.float32)\n",
    "            \n",
    "    return graph, demand, distance, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "8ad751fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.graphs = []\n",
    "        self.demands = []\n",
    "        self.distances = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            processed = pre_process_graph(data[i])\n",
    "            \n",
    "            self.graphs.append(processed[0])\n",
    "            self.demands.append(processed[1])\n",
    "            self.distances.append(processed[2])\n",
    "            self.labels.append(processed[3])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        graph = self.graphs[idx]\n",
    "        demand = self.demands[idx]\n",
    "        distance = self.distances[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return graph, demand, distance, label\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"GraphDataset(graphs={len(self.graphs)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e503b7",
   "metadata": {},
   "source": [
    "The dataset should still be split in a train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "390f7d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(GraphDataset(graphs=1800), GraphDataset(graphs=200))"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_off = int(len(graphs) * 0.9)\n",
    "\n",
    "graph_train_dataset = GraphDataset(graphs[:cut_off])\n",
    "graph_test_dataset = GraphDataset(graphs[cut_off:])\n",
    "\n",
    "graph_train_dataset, graph_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "1a36435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(graph_train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(graph_test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b5bf0",
   "metadata": {},
   "source": [
    "## Attention Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "24f45587",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionEncoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x, neighbor):\n",
    "        '''\n",
    "        @param x: (batch_size, node_num, hidden_dim)\n",
    "        @param neighbor: (batch_size, node_num, k, hidden_dim)\n",
    "        '''\n",
    "        # scaled dot-product attention\n",
    "        x = x.unsqueeze(2)\n",
    "        neighbor = neighbor.permute(0, 1, 3, 2)\n",
    "        attn_score = F.softmax(torch.matmul(x, neighbor) / np.sqrt(self.hidden_dim), dim=-1) # (batch_size, node_num, 1, k)\n",
    "        weighted_neighbor = attn_score * neighbor\n",
    "        \n",
    "        # aggregation\n",
    "        agg = x.squeeze(2) + torch.sum(weighted_neighbor, dim=-1)\n",
    "        \n",
    "        return agg\n",
    "\n",
    "class AttentionPointer(nn.Module):\n",
    "    def __init__(self, hidden_dim, use_tanh=False, use_cuda=False):\n",
    "        super(AttentionPointer, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_tanh = use_tanh\n",
    "\n",
    "        self.project_hidden = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.project_x = nn.Conv1d(hidden_dim, hidden_dim, 1, 1)\n",
    "        self.C = 10\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        v = torch.FloatTensor(hidden_dim)\n",
    "        if use_cuda:\n",
    "            v = v.cuda()\n",
    "        self.v = nn.Parameter(v)\n",
    "        self.v.data.uniform_(-(1. / math.sqrt(hidden_dim)) , 1. / math.sqrt(hidden_dim))\n",
    "\n",
    "    def forward(self, hidden, x):\n",
    "        '''\n",
    "        @param hidden: (batch_size, hidden_dim)\n",
    "        @param x: (node_num, batch_size, hidden_dim)\n",
    "        '''\n",
    "        x = x.permute(1, 2, 0)\n",
    "        q = self.project_hidden(hidden).unsqueeze(2)  # batch_size x hidden_dim x 1\n",
    "        e = self.project_x(x)  # batch_size x hidden_dim x node_num \n",
    "        # expand the hidden by node_num\n",
    "        # batch_size x hidden_dim x node_num\n",
    "        expanded_q = q.repeat(1, 1, e.size(2)) \n",
    "        # batch x 1 x hidden_dim\n",
    "        v_view = self.v.unsqueeze(0).expand(expanded_q.size(0), len(self.v)).unsqueeze(1)\n",
    "        # (batch_size x 1 x hidden_dim) * (batch_size x hidden_dim x node_num)\n",
    "        u = torch.bmm(v_view, self.tanh(expanded_q + e)).squeeze(1)\n",
    "        if self.use_tanh:\n",
    "            logits = self.C * self.tanh(u)\n",
    "        else:\n",
    "            logits = u  \n",
    "        return e, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18e4fa8",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "2236694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 node_hidden_dim,\n",
    "                 edge_hidden_dim,\n",
    "                 gcn_num_layers,\n",
    "                 k):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.node_hidden_dim = node_hidden_dim\n",
    "        self.edge_hidden_dim = edge_hidden_dim\n",
    "        self.gcn_num_layers = gcn_num_layers\n",
    "        self.k = k\n",
    "        \n",
    "        self.W1 = nn.Linear(2, self.node_hidden_dim)      # node_W1\n",
    "        self.W2 = nn.Linear(2, self.node_hidden_dim // 2) # node_W2\n",
    "        self.W3 = nn.Linear(1, self.node_hidden_dim // 2) # node_W3\n",
    "        self.W4 = nn.Linear(1, self.edge_hidden_dim // 2) # edge_W4\n",
    "        self.W5 = nn.Linear(1, self.edge_hidden_dim // 2) # edge_W5\n",
    "        \n",
    "        self.node_embedding = nn.Linear(self.node_hidden_dim, self.node_hidden_dim, bias=False) # Eq5\n",
    "        self.edge_embedding = nn.Linear(self.edge_hidden_dim, self.edge_hidden_dim, bias=False) # Eq6\n",
    "\n",
    "        self.gcn_layers = nn.ModuleList([GCNLayer(self.node_hidden_dim) for i in range(self.gcn_num_layers)])\n",
    "        \n",
    "        # Concat of the data (OWN)\n",
    "        num_nodes = 11\n",
    "        num_classes = 5\n",
    "        \n",
    "        self.final = nn.Linear(node_hidden_dim * num_nodes + edge_hidden_dim * num_nodes ** 2, num_nodes * num_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def adjacency(self, m):\n",
    "        '''\n",
    "        @param m: distance (node_num, node_num)\n",
    "        '''\n",
    "        a = torch.zeros_like(m)\n",
    "        idx = torch.argsort(m, dim=1)[:, 1:(self.k+1)]\n",
    "        a.scatter_(1, idx, 1)\n",
    "        a.fill_diagonal_(-1)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def find_neighbors(self, m):\n",
    "        ''' find index of neighbors for each node\n",
    "        @param m: distance (batch_size, node_num, node_num)\n",
    "        '''\n",
    "        neighbor_idx = []\n",
    "        for i in range(m.shape[0]):\n",
    "            idx = torch.argsort(m[i, :, :], dim=1)[:, 1:(self.k+1)].numpy()\n",
    "            neighbor_idx.append(idx)\n",
    "        return torch.LongTensor(neighbor_idx).to(device)\n",
    "\n",
    "    def forward(self, x_c, x_d, m):\n",
    "        '''\n",
    "        @param x_c: coordination (batch_size, node_num(N+1), 2)\n",
    "        @param x_d: demand (batch_size, node_num(N+1))\n",
    "        @param m: distance (batch_size, node_num(N+1), node_num(N+1))\n",
    "        '''\n",
    "        # Eq 2\n",
    "        x0 = self.relu(self.W1(x_c[:, :1, :])) # (batch_size, 1, node_hidden_dim)\n",
    "        xi = self.relu(torch.cat((self.W2(x_c[:, 1:, :]), self.W3(x_d.unsqueeze(2)[:, 1:, :])), dim=-1)) # (batch_size, node_num(N), node_hidden_dim)\n",
    "        x = torch.cat((x0, xi), dim=1)\n",
    "        # Eq 3\n",
    "        a = torch.Tensor([self.adjacency(m[i, :, :]).numpy() for i in range(m.shape[0])]).to(device)\n",
    "        # Eq 4\n",
    "        y = self.relu(torch.cat((self.W4(m.unsqueeze(3)), self.W5(a.unsqueeze(3))), dim=-1))\n",
    "        # Eq 5\n",
    "        h_node = self.node_embedding(x)\n",
    "        # Eq 6\n",
    "        h_edge = self.edge_embedding(y)\n",
    "\n",
    "        # index of neighbors\n",
    "        N = self.find_neighbors(m)\n",
    "\n",
    "        # GCN layers\n",
    "        for gcn_layer in self.gcn_layers:\n",
    "            h_node, h_edge = gcn_layer(h_node, h_edge, N)\n",
    "            \n",
    "        # Merge together (OWN)\n",
    "        batch_size = h_node.size(0)\n",
    "        h = torch.cat((h_node.reshape(batch_size, -1), h_edge.reshape(batch_size, -1)), dim=1)\n",
    "        \n",
    "        f = self.final(h)\n",
    "#         return h_node, h_edge\n",
    "        return f\n",
    "\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(GCNLayer, self).__init__()\n",
    "\n",
    "        # node GCN layers\n",
    "        self.W_node = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.V_node_in = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.V_node = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.attn = AttentionEncoder(hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ln1_node = nn.LayerNorm(hidden_dim)\n",
    "        self.ln2_node = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # edge GCN layers\n",
    "        self.W_edge = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.V_edge_in = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.V_edge = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.W1_edge = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W2_edge = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W3_edge = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ln1_edge = nn.LayerNorm(hidden_dim)\n",
    "        self.ln2_edge = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x, e, neighbor_index):\n",
    "        '''\n",
    "        @param x: (batch_size, node_num(N+1), node_hidden_dim)\n",
    "        @param e: (batch_size, node_num(N+1), node_num(N+1), edge_hidden_dim)\n",
    "        @param neighbor_index: (batch_size, node_num(N+1), k)\n",
    "        '''\n",
    "        # node embedding\n",
    "        batch_size, node_num = x.size(0), x.size(1)\n",
    "        node_hidden_dim = x.size(-1)\n",
    "        t = x.unsqueeze(1).repeat(1, node_num, 1, 1)\n",
    "\n",
    "        neighbor_index = neighbor_index.unsqueeze(3).repeat(1, 1, 1, node_hidden_dim)\n",
    "        neighbor = t.gather(2, neighbor_index)\n",
    "        neighbor = neighbor.view(batch_size, node_num, -1, node_hidden_dim)\n",
    "        \n",
    "        # Eq 7/9\n",
    "        h_nb_node = self.ln1_node(x + self.relu(self.W_node(self.attn(x, neighbor))))\n",
    "        # Eq 12, Eq 8\n",
    "        h_node = self.ln2_node(h_nb_node + self.relu(self.V_node(torch.cat([self.V_node_in(x), h_nb_node], dim=-1))))\n",
    "\n",
    "        # edge embedding\n",
    "        x_from = x.unsqueeze(2).repeat(1, 1, node_num, 1)\n",
    "        x_to = x.unsqueeze(1).repeat(1, node_num, 1, 1)\n",
    "        # Eq 7/10, Eq 11\n",
    "        h_nb_edge = self.ln1_edge(e + self.relu(self.W_edge(self.W1_edge(e) + self.W2_edge(x_from) + self.W3_edge(x_to))))\n",
    "        # Eq 13, Eq 8\n",
    "        h_edge = self.ln2_edge(h_nb_edge + self.relu(self.V_edge(torch.cat((self.V_edge_in(e), h_nb_edge), dim=-1))))\n",
    "\n",
    "        return h_node, h_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "3073a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE_HIDDEN = 2\n",
    "EDGE_HIDDEN = 2\n",
    "GCN_LAYER = 2\n",
    "k = 1\n",
    "\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.96\n",
    "\n",
    "model = GCN(node_hidden_dim=NODE_HIDDEN,\n",
    "            edge_hidden_dim=EDGE_HIDDEN,\n",
    "            gcn_num_layers=GCN_LAYER,\n",
    "            k=k).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=learning_rate,\n",
    "                             weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "608eb976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    out = out.numpy()\n",
    "    labels = labels.numpy()\n",
    "    \n",
    "    acc = np.sum(out == labels, axis=1) / out.shape[1]\n",
    "    \n",
    "    return np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "1e2ef678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x_c, x_d, m, y):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(x_c, x_d, m)\n",
    "        out = out.reshape(out.size(0), 5, -1)\n",
    "        out = out.softmax(1)\n",
    "        out = out.argmax(1)\n",
    "\n",
    "        return accuracy(out, y), out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "24eedb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(x_c, x_d, m, y):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(x_c, x_d, m)\n",
    "    out = out.reshape(out.size(0), 5, -1)\n",
    "    \n",
    "    loss = criterion(out, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "70c8db94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "    \n",
    "        for x_c, x_d, m, y in train_dataloader:\n",
    "            x_c, x_d, m, y = x_c.to(device), x_d.to(device), m.to(device), y.to(device)\n",
    "\n",
    "            loss = train_one_epoch(x_c, x_d, m, y)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "5e03d1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00, Loss: 1.5517\n",
      "Epoch: 01, Loss: 1.5084\n",
      "Epoch: 02, Loss: 1.4073\n",
      "Epoch: 03, Loss: 1.3666\n",
      "Epoch: 04, Loss: 1.3287\n",
      "Epoch: 05, Loss: 1.2670\n",
      "Epoch: 06, Loss: 1.3302\n",
      "Epoch: 07, Loss: 1.3111\n",
      "Epoch: 08, Loss: 1.2942\n",
      "Epoch: 09, Loss: 1.3153\n",
      "Epoch: 10, Loss: 1.2277\n",
      "Epoch: 11, Loss: 1.1901\n",
      "Epoch: 12, Loss: 1.3378\n",
      "Epoch: 13, Loss: 1.2756\n",
      "Epoch: 14, Loss: 1.2316\n",
      "Epoch: 15, Loss: 1.2670\n",
      "Epoch: 16, Loss: 1.3305\n",
      "Epoch: 17, Loss: 1.2406\n",
      "Epoch: 18, Loss: 1.3305\n",
      "Epoch: 19, Loss: 1.3180\n",
      "Epoch: 20, Loss: 1.2653\n",
      "Epoch: 21, Loss: 1.1840\n",
      "Epoch: 22, Loss: 1.1894\n",
      "Epoch: 23, Loss: 1.1039\n",
      "Epoch: 24, Loss: 1.1247\n",
      "Epoch: 25, Loss: 1.1216\n",
      "Epoch: 26, Loss: 1.0895\n",
      "Epoch: 27, Loss: 1.0272\n",
      "Epoch: 28, Loss: 0.9986\n",
      "Epoch: 29, Loss: 1.3137\n",
      "Epoch: 30, Loss: 1.1698\n",
      "Epoch: 31, Loss: 1.1401\n",
      "Epoch: 32, Loss: 1.1622\n",
      "Epoch: 33, Loss: 1.1107\n",
      "Epoch: 34, Loss: 1.2913\n",
      "Epoch: 35, Loss: 1.0609\n",
      "Epoch: 36, Loss: 1.0638\n",
      "Epoch: 37, Loss: 1.1781\n",
      "Epoch: 38, Loss: 1.1915\n",
      "Epoch: 39, Loss: 1.1096\n",
      "Epoch: 40, Loss: 1.0930\n",
      "Epoch: 41, Loss: 1.0276\n",
      "Epoch: 42, Loss: 1.1284\n",
      "Epoch: 43, Loss: 1.2438\n",
      "Epoch: 44, Loss: 1.1537\n",
      "Epoch: 45, Loss: 1.1241\n",
      "Epoch: 46, Loss: 1.1740\n",
      "Epoch: 47, Loss: 1.1022\n",
      "Epoch: 48, Loss: 1.0786\n",
      "Epoch: 49, Loss: 1.2202\n"
     ]
    }
   ],
   "source": [
    "train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "edd2b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_c, x_d, m, y = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "2fbf3784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.42471590909090906,\n",
       " tensor([[0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1]]))"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(x_c, x_d, m, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78453d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
