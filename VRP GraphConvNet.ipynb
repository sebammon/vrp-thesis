{"cells":[{"cell_type":"markdown","id":"255b204d","metadata":{"id":"255b204d"},"source":["# VRP GraphNet\n","\n","model inputs from the paper:\n","\n","| Variable             | Meaning                           | Dimensions                |\n","|----------------------|-----------------------------------|---------------------------|\n","| batch_edges          | Adj matrix special connections*   | B x num_nodes x num_nodes |\n","| batch_edges_values   | Distance Matrix                   | B x num_nodes x num_nodes |\n","| batch_edges_target   | Target adj matrix                 | B x num_nodes x num_nodes |\n","| batch_nodes          | Ones vector                       | B x num_nodes             |\n","| batch_nodes_coord    | Coordinates                       | B x num_nodes x 2         |\n","| *batch_nodes_target* | Value represents ordering in tour | B x num_nodes             |\n","\n","\n","*special connections:\n","* 1 - k-nearest neighbour\n","* 2 - self connections\n","* 0 - otherwise"]},{"cell_type":"code","execution_count":null,"outputs":[],"source":["try:\n","    from google.colab import drive\n","\n","    drive.mount('/content/gdrive')\n","\n","    %cd gdrive/My Drive/vrp-thesis\n","    %pip install -r requirements-colab.txt\n","    IN_COLAB = True\n","except:\n","    IN_COLAB = False"],"metadata":{"id":"UdOXJtQdw4OV"},"id":"UdOXJtQdw4OV"},{"cell_type":"code","source":["if IN_COLAB:\n","    %reload_ext tensorboard\n","    %tensorboard --logdir runs"],"metadata":{"id":"req4SgVPxtrA"},"id":"req4SgVPxtrA","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"aa54dee2","metadata":{"id":"aa54dee2"},"outputs":[],"source":["from pathlib import Path\n","\n","import numpy as np\n","import seaborn as sns\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from model import GraphNet\n","from utils import load_config, get_metrics, get_device, save_checkpoint, _n, DotDict, load_checkpoint\n","from utils.beam_search import BeamSearch\n","from utils.data import load_and_split_dataset, process_datasets, sparse_matrix_from_routes, distance_from_sparse_matrix, distance_from_adj_matrix\n","\n","sns.set_theme()"]},{"cell_type":"markdown","id":"917b562d","metadata":{"id":"917b562d"},"source":["## Load datasets"]},{"cell_type":"code","execution_count":null,"id":"df7a9efa","metadata":{"id":"df7a9efa"},"outputs":[],"source":["dsets = load_and_split_dataset(\"data/vrp_20_3s_random_depot.pkl\", test_size=500)\n","train_dataset, test_dataset = process_datasets(dsets, k=6)\n","\n","print(len(train_dataset), len(test_dataset))"]},{"cell_type":"markdown","id":"b7aed5e1","metadata":{"id":"b7aed5e1"},"source":["## Basic Config"]},{"cell_type":"code","execution_count":null,"id":"4347cb56","metadata":{"id":"4347cb56"},"outputs":[],"source":["device = get_device()\n","print(\"Device\", device)"]},{"cell_type":"code","execution_count":null,"id":"87e02b64","metadata":{"id":"87e02b64"},"outputs":[],"source":["default_config = load_config(hidden_dim=32, num_gcn_layers=5, num_mlp_layers=3,\n","                             learning_rate=0.001, train_batch_size=128, test_batch_size=256, num_epochs=50)\n","default_config"]},{"cell_type":"code","execution_count":null,"id":"27706ed0","metadata":{"id":"27706ed0"},"outputs":[],"source":["train_dataloader = DataLoader(train_dataset, batch_size=default_config.train_batch_size,\n","                              shuffle=True)\n","\n","model = GraphNet(default_config).to(device)"]},{"cell_type":"markdown","id":"ca258e29","metadata":{"id":"ca258e29"},"source":["## Test Forward Pass"]},{"cell_type":"code","execution_count":null,"id":"84b13064","metadata":{"id":"84b13064"},"outputs":[],"source":["features, _ = next(iter(train_dataloader))\n","\n","y_pred = model.forward(features[\"node_features\"].to(device),\n","                       features[\"dist_matrix\"].to(device),\n","                       features[\"edge_feat_matrix\"].to(device))\n","\n","y_pred.shape"]},{"cell_type":"markdown","id":"d29093f8","metadata":{"id":"d29093f8"},"source":["## Validation loop"]},{"cell_type":"code","execution_count":null,"id":"ea1009ac","metadata":{"id":"ea1009ac"},"outputs":[],"source":["def adj_matrix_from_routes(routes, num_nodes):\n","    \"\"\"\n","    Converts a batch of routes to a batch of adjacency matrices.\n","    :param routes: Batch of route\n","    :param num_nodes: Number of nodes\n","    :return: Batch of adjacency matrices\n","    \"\"\"\n","    routes_rolled = np.roll(routes, -1)\n","    non_zero_indecies = np.stack((routes, routes_rolled), 2)\n","\n","    matrix = np.zeros((routes.shape[0], num_nodes, num_nodes))\n","\n","    for i, indecies in enumerate(non_zero_indecies):\n","        matrix[i, indecies[:, 0], indecies[:, 1]] = 1\n","        matrix[i, indecies[:, 1], indecies[:, 0]] = 1\n","\n","    return matrix\n","    \n","\n","def count_violations(tours, demands):\n","    \"\"\"\n","    Count the number of violations per tour given the customer demands\n","    :param tours: (b, n) array of tours\n","    :param demands: (b, m) array of demands\n","    :return: (b,) array of count of violations\n","    \"\"\"\n","    violations = np.zeros(tours.shape[0])\n","\n","    for i in range(tours.shape[0]):\n","        count = 0\n","        running_load = 0\n","\n","        for j in range(tours.shape[1]):\n","            node = tours[i, j]\n","            running_load += demands[i, node]\n","\n","            if node == 0 or j == tours.shape[1] - 1:\n","                if running_load > 1:\n","                    count += 1\n","                running_load = 0\n","\n","        violations[i] = count\n","\n","    return violations\n","\n","\n","def shortest_valid_tour(y_preds, batch_dist_matrix, batch_node_features,\n","                        num_vehicles, beam_width=1024):\n","    # Move tensors to CPU for faster computation (due to loops and compare ops)\n","    y_preds = y_preds.cpu()\n","    batch_dist_matrix = batch_dist_matrix.cpu().numpy()\n","    batch_node_features = batch_node_features.cpu().numpy()\n","\n","    y_preds = y_preds[..., 1]\n","\n","    beamsearch = BeamSearch(y_preds, beam_width=beam_width, num_vehicles=num_vehicles)\n","    beamsearch.search()\n","\n","    shortest_tour = np.zeros((beamsearch.batch_size, len(beamsearch.next_nodes)))\n","    shortest_tour_length = np.full((beamsearch.batch_size,), np.inf)\n","    max_violations = np.full((beamsearch.batch_size,), np.inf)\n","\n","    for b in range(beamsearch.beam_width):\n","        # can probably be improved by moving to separate loop\n","        current_tour = beamsearch.get_beam(b)\n","        current_tour = current_tour.numpy()\n","\n","        __adj_matrix = adj_matrix_from_routes(current_tour, batch_dist_matrix.shape[-1])\n","        tour_length = distance_from_adj_matrix(__adj_matrix, batch_dist_matrix)\n","        violations = count_violations(current_tour, batch_node_features[..., 2])\n","\n","        for i in range(beamsearch.batch_size):\n","            # there are less violations, so we take it\n","            if violations[i] <  max_violations[i]:\n","                shortest_tour[i] = current_tour[i]\n","                shortest_tour_length[i] = tour_length[i]\n","                max_violations[i] = violations[i]\n","\n","            # same number of violations, take the shorter distance\n","            elif violations[i] == max_violations[i]:\n","                if tour_length[i] < shortest_tour_length[i]:\n","                    shortest_tour[i] = current_tour[i]\n","                    shortest_tour_length[i] = tour_length[i]\n","\n","    return shortest_tour_length, shortest_tour, max_violations\n","\n","def probable_tour_lengths(y_preds, batch_dist_matrix, num_vehicles, beam_width=1024):\n","    # only keep the probability of selecting the edge\n","    y_preds = y_preds[..., 1]\n","\n","    beamsearch = BeamSearch(y_preds, beam_width=beam_width, num_vehicles=num_vehicles)\n","    beamsearch.search()\n","\n","    tours = beamsearch.get_beam(0)\n","    tours = tours.cpu().numpy()\n","\n","    __adj_matrix = adj_matrix_from_routes(tours, batch_dist_matrix.shape[-1])\n","    tour_lengths = distance_from_adj_matrix(__adj_matrix, batch_dist_matrix)\n","\n","    return tour_lengths, tours\n","\n","\n","def greedy_tour_lengths(y_preds, batch_dist_matrix, num_vehicles):\n","    # only keep the probability of selecting the edge\n","    y_preds = y_preds[..., 1]\n","\n","    beamsearch = BeamSearch(y_preds, beam_width=1, num_vehicles=num_vehicles, allow_consecutive_visits=False)\n","    beamsearch.search()\n","\n","    # get most probable tours (index = 0)\n","    tours = beamsearch.get_beam(0)\n","    tours = tours.cpu().numpy()\n","\n","    __adj_matrix = adj_matrix_from_routes(tours, batch_dist_matrix.shape[-1])\n","    tour_lengths = distance_from_adj_matrix(__adj_matrix, batch_dist_matrix)\n","\n","    return tour_lengths, tours\n","\n","\n","def eval_model(batch_node_features, batch_dist_matrix, batch_edge_features, model):\n","    model.eval()\n","\n","    with torch.no_grad():\n","        preds = model(batch_node_features, batch_dist_matrix, batch_edge_features)\n","        preds = F.softmax(preds, dim=3)\n","\n","        return preds\n","\n","\n","def validate(dataloader, model, criterion):\n","    running_loss = 0\n","    running_tour_lengths = []\n","    running_tour_violations = []\n","    targets = []\n","    predictions = []\n","\n","    for batch_features, batch_targets in dataloader:\n","        batch_node_features = batch_features[\"node_features\"].to(device)\n","        batch_dist_matrix = batch_features[\"dist_matrix\"].to(device)\n","        batch_edge_features = batch_features[\"edge_feat_matrix\"].to(device)\n","        # is this required??\n","        batch_num_vehicles = batch_features[\"num_vehicles\"].to(device)\n","        batch_targets = batch_targets.to(device)\n","\n","        y_preds = eval_model(batch_node_features, batch_dist_matrix, batch_edge_features,\n","                             model=model)\n","\n","        # Loss\n","        loss = get_loss(y_preds, batch_targets, criterion)\n","        running_loss += loss.item()\n","\n","        # Tour lengths (mean per batch)\n","        for vehicles in torch.unique(batch_num_vehicles):\n","            mask = batch_num_vehicles == vehicles\n","\n","            tour_lengths, tours = greedy_tour_lengths(y_preds[mask], batch_dist_matrix[mask],\n","                                                      num_vehicles=vehicles)\n","            violations = count_violations(tours, batch_node_features[mask])\n","\n","            running_tour_violations.extend(violations.cpu().numpy())\n","            running_tour_lengths.extend(tour_lengths.cpu().numpy())\n","\n","        y_preds = y_preds.argmax(dim=3)\n","        y_preds = y_preds.cpu().numpy()\n","\n","        targets.append(batch_targets.cpu().numpy())\n","        predictions.append(y_preds)\n","\n","    targets = np.concatenate(targets)\n","    predictions = np.concatenate(predictions)\n","    mean_running_loss = running_loss / len(dataloader)\n","\n","    running_tour_lengths = np.mean(running_tour_lengths)\n","    running_tour_violations = np.mean(running_tour_violations)\n","\n","    return targets, predictions, mean_running_loss, running_tour_lengths, running_tour_violations"]},{"cell_type":"markdown","id":"9e8e3f97","metadata":{"id":"9e8e3f97"},"source":["## Training Loop"]},{"cell_type":"code","execution_count":null,"id":"725e0daf","metadata":{"id":"725e0daf"},"outputs":[],"source":["def get_loss(preds, targets, criterion):\n","    preds_perm = preds.permute(0, 3, 1, 2)\n","\n","    return criterion(preds_perm, targets)\n","\n","\n","def train_one_epoch(dataloader, model, optimizer, criterion):\n","    running_loss = 0\n","\n","    model.train()\n","\n","    for batch_idx, (batch_features, batch_targets) in enumerate(dataloader):\n","        optimizer.zero_grad()\n","\n","        batch_node_features = batch_features[\"node_features\"].to(device)\n","        batch_dist_matrix = batch_features[\"dist_matrix\"].to(device)\n","        batch_edge_features = batch_features[\"edge_feat_matrix\"].to(device)\n","        batch_targets = batch_targets.to(device)\n","\n","        preds = model(batch_node_features, batch_dist_matrix, batch_edge_features)\n","        loss = get_loss(preds, batch_targets, criterion)\n","\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    return running_loss\n","\n","\n","def train(num_epochs, train_dl, test_dl, model, optimizer, criterion, writer):\n","    best_loss = np.inf\n","\n","    for epoch in range(num_epochs):\n","        # Train\n","        running_loss = train_one_epoch(train_dl, model=model, optimizer=optimizer, criterion=criterion)\n","\n","        # Losses\n","        epoch_loss = running_loss / len(train_dl)\n","\n","        # Validation Metrics\n","        targets, predictions, validation_loss, tour_length, violations = validate(test_dl, model=model,\n","                                                                                  criterion=criterion)\n","        metrics = get_metrics(targets, predictions)\n","\n","        writer.add_scalar(\"Metrics/accuracy\", metrics.acc, epoch)\n","        writer.add_scalar(\"Metrics/bal. accuracy\", metrics.bal_acc, epoch)\n","        writer.add_scalar(\"Metrics/precision\", metrics.precision, epoch)\n","        writer.add_scalar(\"Metrics/recall\", metrics.recall, epoch)\n","        writer.add_scalar(\"Metrics/f1 score\", metrics.f1_score, epoch)\n","        writer.add_scalar(\"Metrics/tour length\", tour_length, epoch)\n","        writer.add_scalar(\"Metrics/violations\", violations, epoch)\n","\n","        writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n","        writer.add_scalar(\"Loss/test\", validation_loss, epoch)\n","\n","        # Save (validation) checkpoint\n","        if validation_loss < best_loss:\n","            best_loss = validation_loss\n","            save_checkpoint(writer.log_dir / \"best_validation_loss_model.pt\",\n","                            model=model, optimizer=optimizer,\n","                            epoch=epoch, config={**config}, train_loss=epoch_loss, test_loss=validation_loss)\n","\n","        # Save (epoch) checkpoint\n","        save_checkpoint(writer.log_dir / \"last_epoch_model.pt\",\n","                        model=model, optimizer=optimizer,\n","                        epoch=epoch, config={**config}, train_loss=epoch_loss, test_loss=validation_loss)\n","\n","        print(f'Epoch: {epoch:02d}, Loss: {epoch_loss:.4f}')"]},{"cell_type":"markdown","source":["## Baseline Model"],"metadata":{"collapsed":false,"id":"nzjmHXUiw4Oj"},"id":"nzjmHXUiw4Oj"},{"cell_type":"code","execution_count":null,"id":"483b5b2d","metadata":{"scrolled":false,"id":"483b5b2d"},"outputs":[],"source":["LOG_DIR = Path(f\"runs/exp_baseline_3\")\n","\n","config = load_config(**default_config)\n","# config.hidden_dim = 128\n","# config.gcn_layers = 10\n","config.num_epochs = 10\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=config.train_batch_size, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=config.test_batch_size, shuffle=True)\n","\n","torch.manual_seed(0)\n","\n","edge_class_weights = train_dataset.class_weights().to(device)\n","model = GraphNet(config).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n","criterion = nn.CrossEntropyLoss(edge_class_weights)\n","\n","writer = SummaryWriter(log_dir=LOG_DIR)\n","\n","train(config.num_epochs, train_dl=train_dataloader, test_dl=test_dataloader,\n","      model=model, optimizer=optimizer, criterion=criterion, writer=writer)\n","writer.flush()\n","writer.close()"]},{"cell_type":"markdown","source":["## Plot Results"],"metadata":{"collapsed":false,"id":"wA0uscGhw4Ok"},"id":"wA0uscGhw4Ok"},{"cell_type":"code","source":["MODEL_PATH = Path(\"runs/exp_baseline_2\")\n","\n","checkpoint = load_checkpoint(MODEL_PATH / \"last_epoch_model.pt\")\n","config = DotDict(checkpoint['config'])\n","model = GraphNet(config).to(device)\n","\n","model.load_state_dict(checkpoint['model_state_dict'])"],"metadata":{"id":"ocSs2MglapBF"},"id":"ocSs2MglapBF","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"47f6ec67","metadata":{"id":"47f6ec67"},"outputs":[],"source":["test_dataloader = DataLoader(test_dataset, batch_size=config.test_batch_size, shuffle=True)\n","batch_features, batch_targets = next(iter(test_dataloader))"]},{"cell_type":"code","source":["batch_node_features = batch_features[\"node_features\"].to(device)\n","batch_dist_matrix = batch_features[\"dist_matrix\"].to(device)\n","batch_edge_features = batch_features[\"edge_feat_matrix\"].to(device)\n","batch_num_vehicles = batch_features[\"num_vehicles\"].to(device)\n","batch_targets = batch_targets.to(device)\n","\n","preds = eval_model(batch_node_features, batch_dist_matrix, batch_edge_features,\n","                   model=model)"],"metadata":{"id":"r_k1VrpzbT57"},"id":"r_k1VrpzbT57","execution_count":null,"outputs":[]},{"cell_type":"code","source":["shortest_valid_tour(preds, batch_dist_matrix, batch_node_features, 4, 1024)"],"metadata":{"id":"dwtDRb2RsH9f"},"id":"dwtDRb2RsH9f","execution_count":null,"outputs":[]},{"cell_type":"code","source":["ground_truth_distance = distance_from_adj_matrix(batch_targets, batch_dist_matrix)\n","ground_truth_distance"],"metadata":{"id":"nt7OiP_lLKtU"},"id":"nt7OiP_lLKtU","execution_count":null,"outputs":[]},{"cell_type":"code","source":["choices = np.random.choice(len(batch_targets), 10)"],"metadata":{"id":"xTQIzZvZKMPA"},"id":"xTQIzZvZKMPA","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"outputs":[],"source":["from utils.plot import plot_graph, plot_heatmap, plot_beam_search_tour\n","from utils.data import distance_from_adj_matrix\n","import matplotlib.pyplot as plt\n","\n","ground_truth_distance = distance_from_adj_matrix(batch_targets, batch_dist_matrix)\n","# shortest_distance, tours = shortest_tour_lengths(preds.cpu(), batch_dist_matrix.cpu(), beam_width=1024,\n","#                                                  num_vehicles=torch.max(batch_num_vehicles))\n","\n","# for each \n","\n","for v in torch.unique(batch_num_vehicles):\n","    mask = batch_num_vehicles == v\n","    # route_distance, tours = probable_tour_lengths(preds, batch_dist_matrix, v)\n","    route_distance, tours = greedy_tour_lengths(preds, batch_dist_matrix, v)\n","\n","    for i in np.random.choice(torch.sum(mask).cpu(), 3):\n","        fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n","\n","        plot_graph(_n(batch_node_features[mask][i, :, :2]), _n(batch_targets[mask][i]), ax=ax[0])\n","        plot_heatmap(_n(batch_node_features[mask][i, :, :2]), _n(batch_targets[mask][i]), _n(preds[mask][i][..., 1]), ax=ax[1])\n","        plot_beam_search_tour(_n(batch_node_features[mask][i, :, :2]), _n(batch_targets[mask][i]), _n(tours[mask][i]), ax=ax[2])\n","\n","        ax[0].set_title(f\"Ground truth ({ground_truth_distance[mask][i]:.2f})\")\n","        ax[1].set_title(\"Predictions\")\n","        ax[2].set_title(f\"Shortest tour ({route_distance[mask][i]:.2f})\")\n","        fig.tight_layout()\n","\n","        plt.show()"],"metadata":{"id":"J_xDo0yfw4Ol"},"id":"J_xDo0yfw4Ol"},{"cell_type":"code","execution_count":null,"outputs":[],"source":[],"metadata":{"id":"TIvxuTlfw4Om"},"id":"TIvxuTlfw4Om"}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"colab":{"provenance":[],"gpuType":"T4"}},"nbformat":4,"nbformat_minor":5}