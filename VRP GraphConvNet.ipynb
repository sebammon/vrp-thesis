{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "255b204d",
   "metadata": {
    "id": "255b204d"
   },
   "source": [
    "# VRP GraphNet\n",
    "\n",
    "model inputs from the paper:\n",
    "\n",
    "| Variable             | Meaning                           | Dimensions                |\n",
    "|----------------------|-----------------------------------|---------------------------|\n",
    "| batch_edges          | Adj matrix special connections*   | B x num_nodes x num_nodes |\n",
    "| batch_edges_values   | Distance Matrix                   | B x num_nodes x num_nodes |\n",
    "| batch_edges_target   | Target adj matrix                 | B x num_nodes x num_nodes |\n",
    "| batch_nodes          | Ones vector                       | B x num_nodes             |\n",
    "| batch_nodes_coord    | Coordinates                       | B x num_nodes x 2         |\n",
    "| *batch_nodes_target* | Value represents ordering in tour | B x num_nodes             |\n",
    "\n",
    "\n",
    "*special connections:\n",
    "* 1 - k-nearest neighbour\n",
    "* 2 - self connections\n",
    "* 0 - otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    # noinspection PyUnresolvedReferences\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "\n",
    "    %cd gdrive/My Drive/vrp-thesis\n",
    "    %pip install -r requirements-colab.txt\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False"
   ],
   "metadata": {
    "id": "UdOXJtQdw4OV"
   },
   "id": "UdOXJtQdw4OV"
  },
  {
   "cell_type": "code",
   "source": [
    "if IN_COLAB:\n",
    "    %reload_ext tensorboard\n",
    "    %tensorboard --logdir runs"
   ],
   "metadata": {
    "id": "req4SgVPxtrA"
   },
   "id": "req4SgVPxtrA",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54dee2",
   "metadata": {
    "id": "aa54dee2"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from model import GraphNet\n",
    "from utils import (\n",
    "    load_config,\n",
    "    get_metrics,\n",
    "    get_device,\n",
    "    save_checkpoint,\n",
    "    DotDict,\n",
    "    load_checkpoint,\n",
    "    BeamSearch,\n",
    ")\n",
    "\n",
    "# noinspection PyUnresolvedReferences\n",
    "from utils.data import (\n",
    "    load_and_split_dataset,\n",
    "    process_datasets,\n",
    "    adj_matrix_from_routes,\n",
    "    distance_from_adj_matrix,\n",
    ")\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b562d",
   "metadata": {
    "id": "917b562d"
   },
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7a9efa",
   "metadata": {
    "id": "df7a9efa"
   },
   "outputs": [],
   "source": [
    "dsets = load_and_split_dataset(\"data/vrp_20_3s_random_depot.pkl\", test_size=500)\n",
    "train_dataset, test_dataset = process_datasets(dsets, k=6)\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aed5e1",
   "metadata": {
    "id": "b7aed5e1"
   },
   "source": [
    "## Basic Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4347cb56",
   "metadata": {
    "id": "4347cb56"
   },
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e02b64",
   "metadata": {
    "id": "87e02b64"
   },
   "outputs": [],
   "source": [
    "config = load_config(\n",
    "    hidden_dim=32,\n",
    "    num_gcn_layers=5,\n",
    "    num_mlp_layers=3,\n",
    "    learning_rate=0.001,\n",
    "    train_batch_size=64,\n",
    "    test_batch_size=256,\n",
    "    num_epochs=50,\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27706ed0",
   "metadata": {
    "id": "27706ed0"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=config.train_batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "model = GraphNet(config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca258e29",
   "metadata": {
    "id": "ca258e29"
   },
   "source": [
    "## Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b13064",
   "metadata": {
    "id": "84b13064"
   },
   "outputs": [],
   "source": [
    "features, _ = next(iter(train_dataloader))\n",
    "\n",
    "y_pred = model.forward(\n",
    "    features[\"node_features\"].to(device),\n",
    "    features[\"dist_matrix\"].to(device),\n",
    "    features[\"edge_feat_matrix\"].to(device),\n",
    ")\n",
    "\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29093f8",
   "metadata": {
    "id": "d29093f8"
   },
   "source": [
    "## Validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1009ac",
   "metadata": {
    "id": "ea1009ac"
   },
   "outputs": [],
   "source": [
    "def exceeds_capacity(tour, demand):\n",
    "    loads = np.take(demand, tour)\n",
    "\n",
    "    running_load = 0\n",
    "\n",
    "    for j in range(len(loads)):\n",
    "        running_load += loads[j]\n",
    "\n",
    "        if tour[j] == 0 or j == len(loads) - 1:\n",
    "            if np.round(running_load, 2) > 1.0:\n",
    "                return True\n",
    "            running_load = 0\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_valid(tours, demands, num_nodes):\n",
    "    assert (\n",
    "        tours.shape[0] == demands.shape[0]\n",
    "    ), \"Batch size of tours and demands must match\"\n",
    "    assert isinstance(tours, np.ndarray) and isinstance(\n",
    "        demands, np.ndarray\n",
    "    ), \"tours and demands must be numpy arrays\"\n",
    "\n",
    "    valid = np.ones(tours.shape[0], dtype=bool)\n",
    "\n",
    "    for i, tour in enumerate(tours):\n",
    "        node_visit_count = np.bincount(tour, minlength=num_nodes)\n",
    "        _valid_capacity = not exceeds_capacity(tour, demands[i])\n",
    "        _valid_tour = np.all(node_visit_count[1:] == 1)\n",
    "\n",
    "        valid[i] = _valid_capacity and _valid_tour\n",
    "\n",
    "    return valid\n",
    "\n",
    "\n",
    "def shortest_valid_tour(\n",
    "    y_preds,\n",
    "    batch_dist_matrix,\n",
    "    batch_node_features,\n",
    "    num_vehicles=None,\n",
    "    beam_width=1024,\n",
    "    allow_consecutive_visits=False,\n",
    "):\n",
    "    # Move tensors to CPU for faster computation (due to loops and compare ops)\n",
    "    y_preds = y_preds.cpu()\n",
    "    batch_demands = batch_node_features[..., 2].cpu()\n",
    "\n",
    "    y_preds = y_preds[..., 1]\n",
    "\n",
    "    beamsearch = BeamSearch(\n",
    "        y_preds,\n",
    "        demands=batch_demands if num_vehicles is None else None,\n",
    "        num_vehicles=num_vehicles or 0,\n",
    "        allow_consecutive_visits=allow_consecutive_visits,\n",
    "        beam_width=beam_width,\n",
    "    )\n",
    "    beamsearch.search()\n",
    "\n",
    "    batch_dist_matrix = batch_dist_matrix.cpu().numpy()\n",
    "    batch_demands = batch_demands.numpy()\n",
    "\n",
    "    shortest_tour = np.zeros(\n",
    "        (beamsearch.batch_size, len(beamsearch.next_nodes)), dtype=int\n",
    "    )\n",
    "    shortest_tour_length = np.full((beamsearch.batch_size,), np.inf)\n",
    "\n",
    "    for b in range(beamsearch.beam_width):\n",
    "        current_tour = beamsearch.get_beam(b)\n",
    "        current_tour = current_tour.numpy()\n",
    "\n",
    "        adj_matrix = adj_matrix_from_routes(current_tour, beamsearch.num_nodes)\n",
    "        tour_length = distance_from_adj_matrix(adj_matrix, batch_dist_matrix)\n",
    "        valid = is_valid(current_tour, batch_demands, beamsearch.num_nodes)\n",
    "\n",
    "        for i in range(beamsearch.batch_size):\n",
    "            if valid[i] and tour_length[i] < shortest_tour_length[i]:\n",
    "                shortest_tour[i] = current_tour[i]\n",
    "                shortest_tour_length[i] = tour_length[i]\n",
    "\n",
    "    return shortest_tour, shortest_tour_length\n",
    "\n",
    "\n",
    "def greedy_tour_lengths(y_preds, batch_dist_matrix, batch_node_features):\n",
    "    y_preds = y_preds.cpu()\n",
    "    batch_demands = batch_node_features[..., 2].cpu()\n",
    "\n",
    "    # only keep the probability of selecting the edge\n",
    "    y_preds = y_preds[..., 1]\n",
    "\n",
    "    beamsearch = BeamSearch(\n",
    "        y_preds, demands=batch_demands, beam_width=1, num_vehicles=0\n",
    "    )\n",
    "    beamsearch.search()\n",
    "\n",
    "    # get most probable tours (index = 0)\n",
    "    tours = beamsearch.get_beam(0)\n",
    "\n",
    "    tours = tours.cpu().numpy()\n",
    "    batch_dist_matrix = batch_dist_matrix.cpu().numpy()\n",
    "\n",
    "    __adj_matrix = adj_matrix_from_routes(tours, batch_dist_matrix.shape[-1])\n",
    "    tour_lengths = distance_from_adj_matrix(__adj_matrix, batch_dist_matrix)\n",
    "\n",
    "    return tours, tour_lengths\n",
    "\n",
    "\n",
    "def eval_model(batch_node_features, batch_dist_matrix, batch_edge_features, model):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(batch_node_features, batch_dist_matrix, batch_edge_features)\n",
    "        preds = F.softmax(preds, dim=3)\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "def validate(dataloader, model, criterion):\n",
    "    running_loss = 0\n",
    "    running_tour_lengths = 0\n",
    "    running_opt_gap = 0\n",
    "    running_tour_count = 0\n",
    "    targets = []\n",
    "    predictions = []\n",
    "\n",
    "    for batch_features, batch_targets in dataloader:\n",
    "        batch_node_features = batch_features[\"node_features\"].to(device)\n",
    "        batch_dist_matrix = batch_features[\"dist_matrix\"].to(device)\n",
    "        batch_edge_features = batch_features[\"edge_feat_matrix\"].to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        y_preds = eval_model(\n",
    "            batch_node_features, batch_dist_matrix, batch_edge_features, model=model\n",
    "        )\n",
    "\n",
    "        # Loss\n",
    "        loss = get_loss(y_preds, batch_targets, criterion)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        opt_tour_lengths = distance_from_adj_matrix(batch_targets, batch_dist_matrix)\n",
    "        # Greedy (cap) tour lengths\n",
    "        _, tour_lengths = greedy_tour_lengths(\n",
    "            y_preds, batch_dist_matrix, batch_node_features\n",
    "        )\n",
    "\n",
    "        opt_gap = (tour_lengths / opt_tour_lengths) - 1\n",
    "\n",
    "        running_tour_lengths += tour_lengths.sum()\n",
    "        running_tour_count += len(tour_lengths)\n",
    "        running_opt_gap += opt_gap.sum()\n",
    "\n",
    "        y_preds = y_preds.argmax(dim=3)\n",
    "        y_preds = y_preds.cpu().numpy()\n",
    "\n",
    "        targets.append(batch_targets.cpu().numpy())\n",
    "        predictions.append(y_preds)\n",
    "\n",
    "    targets = np.concatenate(targets)\n",
    "    predictions = np.concatenate(predictions)\n",
    "    mean_running_loss = running_loss / len(dataloader)\n",
    "    mean_tour_lengths = running_tour_lengths / running_tour_count\n",
    "    mean_opt_gap = running_opt_gap / running_tour_count\n",
    "\n",
    "    return (targets, predictions, mean_running_loss, mean_tour_lengths, mean_opt_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e3f97",
   "metadata": {
    "id": "9e8e3f97"
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e0daf",
   "metadata": {
    "id": "725e0daf"
   },
   "outputs": [],
   "source": [
    "def get_loss(preds, targets, criterion):\n",
    "    preds_perm = preds.permute(0, 3, 1, 2)\n",
    "\n",
    "    return criterion(preds_perm, targets)\n",
    "\n",
    "\n",
    "def train_one_epoch(dataloader, model, optimizer, criterion):\n",
    "    running_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (batch_features, batch_targets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_node_features = batch_features[\"node_features\"].to(device)\n",
    "        batch_dist_matrix = batch_features[\"dist_matrix\"].to(device)\n",
    "        batch_edge_features = batch_features[\"edge_feat_matrix\"].to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        preds = model(batch_node_features, batch_dist_matrix, batch_edge_features)\n",
    "        loss = get_loss(preds, batch_targets, criterion)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss\n",
    "\n",
    "\n",
    "def train(num_epochs, train_dl, test_dl, model, optimizer, criterion, writer):\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        running_loss = train_one_epoch(\n",
    "            train_dl, model=model, optimizer=optimizer, criterion=criterion\n",
    "        )\n",
    "\n",
    "        # Losses\n",
    "        epoch_loss = running_loss / len(train_dl)\n",
    "\n",
    "        # Validation Metrics\n",
    "        (\n",
    "            targets,\n",
    "            predictions,\n",
    "            validation_loss,\n",
    "            mean_tour_length,\n",
    "            mean_opt_gap,\n",
    "        ) = validate(test_dl, model=model, criterion=criterion)\n",
    "        metrics = get_metrics(targets, predictions)\n",
    "\n",
    "        writer.add_scalar(\"Metrics/accuracy\", metrics.acc, epoch)\n",
    "        writer.add_scalar(\"Metrics/bal. accuracy\", metrics.bal_acc, epoch)\n",
    "        writer.add_scalar(\"Metrics/precision\", metrics.precision, epoch)\n",
    "        writer.add_scalar(\"Metrics/recall\", metrics.recall, epoch)\n",
    "        writer.add_scalar(\"Metrics/f1 score\", metrics.f1_score, epoch)\n",
    "        writer.add_scalar(\"Metrics/mean tour length\", mean_tour_length, epoch)\n",
    "        writer.add_scalar(\"Metrics/mean opt. gap\", mean_opt_gap, epoch)\n",
    "\n",
    "        writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/test\", validation_loss, epoch)\n",
    "\n",
    "        # Save (validation) checkpoint\n",
    "        if validation_loss < best_loss:\n",
    "            best_loss = validation_loss\n",
    "            save_checkpoint(\n",
    "                writer.log_dir / \"best_validation_loss_model.pt\",\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                epoch=epoch,\n",
    "                config={**config},\n",
    "                train_loss=epoch_loss,\n",
    "                test_loss=validation_loss,\n",
    "            )\n",
    "\n",
    "        # Save (epoch) checkpoint\n",
    "        save_checkpoint(\n",
    "            writer.log_dir / \"last_epoch_model.pt\",\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            config={**config},\n",
    "            train_loss=epoch_loss,\n",
    "            test_loss=validation_loss,\n",
    "        )\n",
    "\n",
    "        print(f\"Epoch: {epoch:02d}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline Model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "nzjmHXUiw4Oj"
   },
   "id": "nzjmHXUiw4Oj"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483b5b2d",
   "metadata": {
    "scrolled": false,
    "id": "483b5b2d"
   },
   "outputs": [],
   "source": [
    "LOG_DIR = Path(f\"runs/exp_baseline_1\")\n",
    "\n",
    "config = load_config(\n",
    "    hidden_dim=16,\n",
    "    num_gcn_layers=5,\n",
    "    num_mlp_layers=3,\n",
    "    learning_rate=0.001,\n",
    "    train_batch_size=64,\n",
    "    test_batch_size=256,\n",
    "    num_epochs=10,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=config.train_batch_size, shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=config.test_batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "edge_class_weights = train_dataset.class_weights().to(device)\n",
    "model = GraphNet(config).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(edge_class_weights)\n",
    "\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "\n",
    "train(\n",
    "    config.num_epochs,\n",
    "    train_dl=train_dataloader,\n",
    "    test_dl=test_dataloader,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    writer=writer,\n",
    ")\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot Results"
   ],
   "metadata": {
    "collapsed": false,
    "id": "wA0uscGhw4Ok"
   },
   "id": "wA0uscGhw4Ok"
  },
  {
   "cell_type": "code",
   "source": [
    "MODEL_PATH = Path(\"runs/exp_baseline_1\")\n",
    "\n",
    "checkpoint = load_checkpoint(MODEL_PATH / \"last_epoch_model.pt\")\n",
    "config = DotDict(checkpoint[\"config\"])\n",
    "model = GraphNet(config).to(device)\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ],
   "metadata": {
    "id": "ocSs2MglapBF"
   },
   "id": "ocSs2MglapBF",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6ec67",
   "metadata": {
    "id": "47f6ec67"
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=config.test_batch_size, shuffle=True\n",
    ")\n",
    "batch_features, batch_targets = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "batch_node_features = batch_features[\"node_features\"].to(device)\n",
    "batch_dist_matrix = batch_features[\"dist_matrix\"].to(device)\n",
    "batch_edge_features = batch_features[\"edge_feat_matrix\"].to(device)\n",
    "batch_targets = batch_targets.to(device)\n",
    "\n",
    "preds = eval_model(\n",
    "    batch_node_features, batch_dist_matrix, batch_edge_features, model=model\n",
    ")"
   ],
   "metadata": {
    "id": "r_k1VrpzbT57"
   },
   "id": "r_k1VrpzbT57",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "routes, distances = shortest_valid_tour(\n",
    "    preds,\n",
    "    batch_dist_matrix,\n",
    "    batch_node_features,\n",
    "    num_vehicles=None,\n",
    "    beam_width=1024,\n",
    "    allow_consecutive_visits=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "1 - np.isinf(distances).sum() / len(distances)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for c in np.random.choice(len(batch_node_features), 5):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    sns.heatmap(batch_targets[c].cpu().numpy(), ax=ax[0])\n",
    "    sns.heatmap(preds[c, ..., 1].cpu().numpy(), ax=ax[1])\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.plot import plot_graph, plot_heatmap, plot_beam_search_tour\n",
    "\n",
    "NUM_SAMPLES = 10\n",
    "\n",
    "actual_distance = distance_from_adj_matrix(batch_targets, batch_dist_matrix)\n",
    "# routes, distances = greedy_tour_lengths(preds, batch_dist_matrix, batch_node_features)\n",
    "routes, distances = shortest_valid_tour(\n",
    "    preds, batch_dist_matrix, batch_node_features, None\n",
    ")\n",
    "\n",
    "actual_distance = actual_distance.cpu().numpy()\n",
    "node_features = batch_node_features.cpu().numpy()\n",
    "targets = batch_targets.cpu().numpy()\n",
    "predictions = preds[..., 1].cpu().numpy()\n",
    "\n",
    "for i in np.random.choice(len(node_features), NUM_SAMPLES):\n",
    "    print(f\"Sample {i}\")\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    plot_graph(node_features[i, :, :2], targets[i], ax=ax[0])\n",
    "    plot_heatmap(node_features[i, :, :2], targets[i], predictions[i], ax=ax[1])\n",
    "    plot_beam_search_tour(node_features[i, :, :2], targets[i], routes[i], ax=ax[2])\n",
    "\n",
    "    ax[0].set_title(f\"Ground truth ({actual_distance[i]:.2f})\")\n",
    "    ax[1].set_title(\"Predictions\")\n",
    "    ax[2].set_title(f\"Shortest tour ({distances[i]:.2f})\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "J_xDo0yfw4Ol"
   },
   "id": "J_xDo0yfw4Ol"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "TIvxuTlfw4Om"
   },
   "id": "TIvxuTlfw4Om"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
