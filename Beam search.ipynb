{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4cf52f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0f1b2186",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width = 10\n",
    "batch_size = 2\n",
    "num_nodes = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8fee8d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "y = torch.randn(batch_size, num_nodes, num_nodes, 2).type(torch.float)\n",
    "y_pred = torch.nn.functional.softmax(y, dim=3)\n",
    "y_pred = y_pred[:, :, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "718f32f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_topk(input_tensor, k, dim=-1, descending=True):\n",
    "    # as a workaround for torch.topk(...) not being stable: https://github.com/pytorch/pytorch/issues/3982\n",
    "    values, indices = torch.sort(input_tensor, dim=dim, descending=descending, stable=True)\n",
    "\n",
    "    return values[..., :k], indices[..., :k]\n",
    "\n",
    "\n",
    "class Beamsearch:\n",
    "    \"\"\"\n",
    "    Beam search procedure class.\n",
    "\n",
    "    References:\n",
    "        [1]: https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/translate/beam.py\n",
    "        [2]: https://github.com/alexnowakvila/QAP_pt/blob/master/src/tsp/beam_search.py\n",
    "        [3]: https://github.com/chaitjo/graph-convnet-tsp/blob/master/utils/beamsearch.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, beam_width, trans_probs, num_vehicles=1, vehicle_capacity=1, random_start=False):\n",
    "        # beam-search parameters\n",
    "        self.beam_width = beam_width\n",
    "\n",
    "        # TODO: Move tensors to GPU device for faster computation\n",
    "        # tensor data types and device\n",
    "        self.device = None\n",
    "        self.float = torch.float32\n",
    "        self.long = torch.int64\n",
    "\n",
    "        # all transition probabilities\n",
    "        self.trans_probs = trans_probs.type(self.float)\n",
    "        self.batch_size = trans_probs.size(0)\n",
    "        self.num_nodes = trans_probs.size(1)\n",
    "        self.num_vehicles = num_vehicles\n",
    "        self.vehicle_capacity = vehicle_capacity\n",
    "\n",
    "        assert len(trans_probs.shape) == 3, \"transition probabilities need to be 3-dimensional\"\n",
    "        assert trans_probs.size(1) == trans_probs.size(2), \"transition probabilities are not square\"\n",
    "\n",
    "        if random_start:\n",
    "            # starting at random nodes\n",
    "            start_nodes = torch.randint(0, self.num_nodes, (self.batch_size, self.beam_width))\n",
    "        else:\n",
    "            # starting at node zero\n",
    "            start_nodes = torch.zeros(self.batch_size, self.beam_width)\n",
    "\n",
    "        self.start_nodes = start_nodes.type(self.long)\n",
    "        self.depot_visits_counter = torch.zeros(self.batch_size, self.beam_width)\n",
    "        self.remaining_capacity = torch.ones(self.batch_size, self.beam_width) * self.vehicle_capacity\n",
    "\n",
    "        # mask for removing visited nodes etc.\n",
    "        self.mask = torch.ones(self.batch_size, self.beam_width, self.num_nodes).type(self.float)\n",
    "\n",
    "        # transition probability scores up-until current timestep\n",
    "        self.scores = torch.zeros(self.batch_size, self.beam_width).type(self.float)\n",
    "\n",
    "        # pointers to parents for each timestep\n",
    "        self.parent_pointer = []\n",
    "\n",
    "        # nodes at each timestep\n",
    "        self.next_nodes = [self.start_nodes]\n",
    "\n",
    "        # start by masking the starting nodes\n",
    "        self.update_mask(self.start_nodes)\n",
    "\n",
    "    def get_current_nodes(self):\n",
    "        \"\"\"\n",
    "        Get the nodes to expand at the current timestep\n",
    "        \"\"\"\n",
    "        current_nodes = self.next_nodes[-1]\n",
    "        current_nodes = current_nodes.unsqueeze(2).expand_as(self.mask)\n",
    "\n",
    "        return current_nodes\n",
    "\n",
    "    @property\n",
    "    def num_iterations(self):\n",
    "        # -1 for num_nodes because we already start at depot\n",
    "        # -1 to offset num_vehicles\n",
    "        return self.num_nodes + self.num_vehicles - 2\n",
    "\n",
    "    def search(self):\n",
    "        \"\"\"\n",
    "        Start beam search\n",
    "        \"\"\"\n",
    "        for step in range(self.num_iterations):\n",
    "            self.step()\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Transition to the next timestep of the beam search\n",
    "        \"\"\"\n",
    "        current_nodes = self.get_current_nodes()\n",
    "        trans_probs = self.trans_probs.gather(1, current_nodes)\n",
    "\n",
    "        if len(self.parent_pointer) == 0:\n",
    "            # first transition, only use the starting nodes\n",
    "            beam_prob = trans_probs\n",
    "            beam_prob[:, 1:] = torch.zeros_like(beam_prob[:, 1:])\n",
    "        else:\n",
    "            # multiply the previous scores (probabilities) with the current ones\n",
    "            expanded_scores = self.scores.unsqueeze(2).expand_as(trans_probs)  # b x beam_width x num_nodes\n",
    "            beam_prob = trans_probs * expanded_scores\n",
    "\n",
    "        # mask out visited nodes\n",
    "        beam_prob = beam_prob * self.mask\n",
    "        beam_prob[..., 0] += 1e-25  # always make the depot slightly available\n",
    "\n",
    "        beam_prob = beam_prob.view(beam_prob.size(0), -1)  # flatten to (b x beam_width * num_nodes)\n",
    "\n",
    "        # get k=beam_width best scores and indices\n",
    "        best_scores, best_score_idxs = stable_topk(beam_prob, k=self.beam_width, dim=1)\n",
    "\n",
    "        self.scores = best_scores\n",
    "        parent_index = torch.floor_divide(best_score_idxs, self.num_nodes).type(self.long)\n",
    "        self.parent_pointer.append(parent_index)\n",
    "\n",
    "        # next nodes\n",
    "        next_node = best_score_idxs - (parent_index * self.num_nodes)  # convert flat indices back to original\n",
    "        self.next_nodes.append(next_node)\n",
    "\n",
    "        # keep masked rows from parents (for next step)\n",
    "        parent_mask = parent_index.unsqueeze(2).expand_as(self.mask)  # (batch_size, beam_size, num_nodes)\n",
    "        self.mask = self.mask.gather(1, parent_mask)\n",
    "\n",
    "        # keep depot counter and capacity from parent (for next step)\n",
    "        self.depot_visits_counter = self.depot_visits_counter.gather(1, parent_index)\n",
    "        self.remaining_capacity = self.remaining_capacity.gather(1, parent_index)\n",
    "\n",
    "        # mask next nodes (newly added nodes)\n",
    "        self.update_mask(next_node)\n",
    "\n",
    "    def update_mask(self, new_nodes):\n",
    "        \"\"\"\n",
    "        Sets indices of new_nodes = 0 in the mask.\n",
    "\n",
    "        Args:\n",
    "            new_nodes: (batch_size, beam_width) of new node indices\n",
    "        \"\"\"\n",
    "        index = torch.arange(0, self.num_nodes, dtype=self.long).expand_as(self.mask)\n",
    "        new_nodes = new_nodes.unsqueeze(2).expand_as(self.mask)\n",
    "\n",
    "        # set the mask = 0 at the new_node_idx positions\n",
    "        visited_nodes_mask = torch.eq(index, new_nodes).type(self.float)\n",
    "        update_mask = 1 - visited_nodes_mask\n",
    "\n",
    "        # increment depot visit counter where visited\n",
    "        self.depot_visits_counter += visited_nodes_mask[..., 0]  # batch_size x beam_width x num_nodes[0]\n",
    "        enable_depot_visit = torch.lt(self.depot_visits_counter, self.num_vehicles).type(self.long)\n",
    "        # enable_depot_visit = enable_depot_visit * update_mask[..., 0]\n",
    "\n",
    "        # TODO:\n",
    "        # 1. reset the remaining capacity when visiting depot\n",
    "        # 2. decrement remaining capacity when visiting new nodes\n",
    "        # 3. mask nodes that don't fit in remaining capacity\n",
    "\n",
    "        self.mask = self.mask * update_mask\n",
    "\n",
    "        # reset depot visit\n",
    "        self.mask[..., 0] = enable_depot_visit\n",
    "\n",
    "    def get_beam(self, beam_idx):\n",
    "        \"\"\"\n",
    "        Construct the beam for the given index\n",
    "\n",
    "        Args:\n",
    "            beam_idx int: Index of the beam to construct (0 = best, ..., n = worst)\n",
    "        \"\"\"\n",
    "        assert len(self.next_nodes) == self.num_iterations + 1\n",
    "\n",
    "        prev_pointer = torch.ones(self.batch_size, 1).type(self.long) * beam_idx\n",
    "        last_node = self.next_nodes[-1].gather(1, prev_pointer)\n",
    "\n",
    "        path = [last_node]\n",
    "\n",
    "        for i in range(len(self.parent_pointer) - 1, -1, -1):\n",
    "            prev_pointer = self.parent_pointer[i].gather(1, prev_pointer)\n",
    "            last_node = self.next_nodes[i].gather(1, prev_pointer)\n",
    "\n",
    "            path.append(last_node)\n",
    "\n",
    "        path = list(reversed(path))\n",
    "        path = torch.cat(path, dim=-1)\n",
    "\n",
    "        return path\n",
    "\n",
    "    def validate(self, beam, batch_idx=None, beam_idx=None):\n",
    "        bin_count = torch.bincount(beam)\n",
    "\n",
    "        assert bin_count[\n",
    "                   0] <= self.num_vehicles, f\"Batch={batch_idx}, beam={beam_idx}: too many depot visits {bin_count[0]} > {self.num_vehicles}\\n{beam}\"\n",
    "        # want them separate for sanity\n",
    "        assert torch.all(bin_count[1:] <= 1), f\"Batch={batch_idx}, beam={beam_idx}: too many node visits\\n{beam}\"\n",
    "        assert torch.all(bin_count[1:] > 0), f\"Batch={batch_idx}, beam={beam_idx}: not all nodes visited\\n{beam}\"\n",
    "\n",
    "    def sanity_check(self):\n",
    "        for batch_idx in range(self.batch_size):\n",
    "            for beam_idx in range(self.beam_width):\n",
    "                beams = self.get_beam(beam_idx)\n",
    "                beam = beams[batch_idx]\n",
    "\n",
    "                self.validate(beam, batch_idx=batch_idx, beam_idx=beam_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "43eac4ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_vehicles = 4\n",
    "\n",
    "beamsearch = Beamsearch(beam_width, trans_probs=y_pred, num_vehicles=num_vehicles)\n",
    "bs = beamsearch\n",
    "\n",
    "beamsearch.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "453c5fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 4, 1, 3, 2, 5, 0, 0, 0],\n",
       "        [0, 3, 2, 0, 5, 0, 1, 0, 4]])"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortest_tours = beamsearch.get_beam(4)\n",
    "shortest_tours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "baaf292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "beamsearch.sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6814c2b4",
   "metadata": {},
   "source": [
    "The problem with the double visits is when the second to last node was the depot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "432fe124",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " tensor([[0, 4, 2, 5, 1, 0, 3, 0, 0],\n",
      "        [0, 3, 2, 0, 5, 0, 1, 4, 0]])\n",
      "\n",
      "1 \n",
      " tensor([[0, 4, 2, 1, 5, 0, 3, 0, 0],\n",
      "        [0, 5, 0, 3, 2, 0, 1, 4, 0]])\n",
      "\n",
      "2 \n",
      " tensor([[0, 4, 2, 3, 5, 0, 1, 0, 0],\n",
      "        [0, 3, 2, 0, 5, 4, 0, 1, 0]])\n",
      "\n",
      "3 \n",
      " tensor([[0, 4, 3, 2, 5, 0, 1, 0, 0],\n",
      "        [0, 1, 3, 2, 0, 5, 0, 4, 0]])\n",
      "\n",
      "4 \n",
      " tensor([[0, 4, 1, 3, 2, 5, 0, 0, 0],\n",
      "        [0, 3, 2, 0, 5, 0, 1, 0, 4]])\n",
      "\n",
      "5 \n",
      " tensor([[0, 4, 2, 1, 3, 5, 0, 0, 0],\n",
      "        [0, 5, 0, 3, 2, 0, 1, 0, 4]])\n",
      "\n",
      "6 \n",
      " tensor([[0, 4, 2, 3, 5, 1, 0, 0, 0],\n",
      "        [0, 3, 2, 0, 1, 4, 5, 0, 0]])\n",
      "\n",
      "7 \n",
      " tensor([[0, 4, 3, 2, 5, 1, 0, 0, 0],\n",
      "        [0, 1, 3, 2, 0, 5, 4, 0, 0]])\n",
      "\n",
      "8 \n",
      " tensor([[0, 4, 5, 1, 3, 2, 0, 0, 0],\n",
      "        [0, 5, 3, 2, 0, 1, 4, 0, 0]])\n",
      "\n",
      "9 \n",
      " tensor([[0, 4, 2, 5, 1, 3, 0, 0, 0],\n",
      "        [0, 3, 2, 0, 5, 4, 1, 0, 0]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for beam_idx in range(beamsearch.beam_width):\n",
    "    beams = beamsearch.get_beam(beam_idx)\n",
    "    print(beam_idx, \"\\n\", beams)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd03112d",
   "metadata": {},
   "source": [
    "## Tour distance\n",
    "\n",
    "The code below can serve as a good starting point for calculating the distance of the tour found so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b1064b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# offset by one to calculate total travel distance\n",
    "idx = (torch.arange(0, shortest_tours.size(1)) + 1) % shortest_tours.size(1)\n",
    "idx = idx.expand_as(shortest_tours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b6edf332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 2, 3, 1, 0],\n",
       "        [3, 2, 4, 1, 0],\n",
       "        [4, 3, 1, 2, 0]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortest_tours.gather(1, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
