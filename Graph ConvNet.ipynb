{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "926cb6ac",
   "metadata": {},
   "source": [
    "# Understanding Graph ConvNet\n",
    "\n",
    "Paper: `Joshi, Laurent, and Bresson, ‘An Efficient Graph Convolutional Network Technique for the Travelling Salesman Problem’.`\n",
    "\n",
    "Open questions:\n",
    "* Difference between Linear and Embedding layer.\n",
    "* Does it make a difference that no two Linear layers were used for the edge layer?\n",
    "* Why are the biases disabled for eq 2 and eq 3 - in the paper they are present?\n",
    "* How does Batch Normalisation work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40b0da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f135ac",
   "metadata": {},
   "source": [
    "## Model Inputs\n",
    "\n",
    "Inputs for the model for the TSP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67961f9c",
   "metadata": {},
   "source": [
    "The following is how the data is generated:\n",
    "\n",
    "```python\n",
    "solver = TSPSolver.from_data(nodes_coord[:,0], nodes_coord[:,1], norm=\"GEO\")  \n",
    "solution = solver.solve()\n",
    "f.write( \" \".join( str(x)+str(\" \")+str(y) for x,y in nodes_coord) )\n",
    "f.write( str(\" \") + str('output') + str(\" \") )\n",
    "f.write( str(\" \").join( str(node_idx+1) for node_idx in solution.tour) )\n",
    "f.write( str(\" \") + str(solution.tour[0]+1) + str(\" \") )\n",
    "f.write( \"\\n\" )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3484392",
   "metadata": {},
   "source": [
    "One line: `x_1, y_1, ... , x_n, y_n output 0 4 1 ... 2 0`\n",
    "\n",
    "The tours are cyclic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e253e",
   "metadata": {},
   "source": [
    "| Variable | Meaning | Dimensions |\n",
    "| -------- | ------- | ---------- |\n",
    "| batch_edges | Adj matrix special connections* | B x num_nodes x num_nodes\n",
    "| batch_edges_values | Distance Matrix | B x num_nodes x num_nodes\n",
    "| batch_edges_target | Target adj matrix | B x num_nodes x num_nodes\n",
    "| batch_nodes | Ones vector | B x num_nodes\n",
    "| batch_nodes_coord | Coordinates | B x num_nodes x 2\n",
    "| *batch_nodes_target* | Value represents ordering in tour | B x num_nodes\n",
    "\n",
    "\n",
    "*special connections:\n",
    "* 1 - k-nearest neighbour\n",
    "* 2 - self connections\n",
    "* 0 - otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "03a623b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some testing data\n",
    "np.random.seed(42)\n",
    "\n",
    "batch_nodes_coord = np.random.random((1, 4, 2))\n",
    "batch_edges_values = np.zeros((1, 4, 4))\n",
    "batch_edges = np.ones((1, 4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8a49d7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37454012, 0.95071431],\n",
       "       [0.73199394, 0.59865848],\n",
       "       [0.15601864, 0.15599452],\n",
       "       [0.05808361, 0.86617615]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_nodes_coord[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "98819ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(batch_nodes_coord)):\n",
    "    node_coords = batch_nodes_coord[i]\n",
    "    dist_matrix = squareform(pdist(node_coords, metric='euclidean'))\n",
    "    batch_edges_values[i] = dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "00e0134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_nodes_coord = torch.tensor(batch_nodes_coord, dtype=torch.float32)\n",
    "batch_edges_values = torch.tensor(batch_edges_values, dtype=torch.float32)\n",
    "batch_edges = torch.tensor(batch_edges, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cbe04c",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "95e2d490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "2fac55e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == NORMALISATION LAYERS ==\n",
    "# The normalisation layers are required because the tensors need to\n",
    "# be transposed for batch normalisation\n",
    "\n",
    "class EdgeNorm(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.batch_norm = nn.BatchNorm2d(hidden_dim, track_running_stats = False)\n",
    "    \n",
    "    def forward(self, e):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            e: Edge features (batch x num_nodes x num_nodes x hidden_dim)\n",
    "        \"\"\"\n",
    "        # transpose because batch norm works on the second dim\n",
    "        e_trans = e.transpose(1, 3).contiguous() # B x hidden_dim x num_nodes x num_nodes\n",
    "        e_trans_batch_norm = self.batch_norm(e_trans)\n",
    "        e_batch_norm = e_trans_batch_norm.transpose(1, 3).contiguous() # B x num_nodes x num_nodes x hidden_dim\n",
    "        \n",
    "        return e_batch_norm\n",
    "\n",
    "class NodeNorm(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim, track_running_stats = False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features (batch x num_nodes x hidden_dim)\n",
    "        \"\"\"\n",
    "        # transpose because batch norm works on the second dim\n",
    "        x_trans = x.transpose(1, 2).contiguous() # B x hidden_dim x num_nodes\n",
    "        x_trans_batch_norm = self.batch_norm(x_trans)\n",
    "        x_batch_norm = x_trans_batch_norm.transpose(1, 2).contiguous() # B x num_nodes x hidden_dim\n",
    "        \n",
    "        return x_batch_norm\n",
    "\n",
    "\n",
    "class EdgeFeatureLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    W_3 e_ij + W_4 (x_i + x_j) <-- currently the case, but should be: W_3 e_ij + W_4 x_i + W_5 x_j\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.W_3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # TODO: Why are not two Linear layers used W_4 and W_5 - does it make a difference?\n",
    "        self.W_4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features (batch x num_nodes x hidden_dim)\n",
    "            e: Edge features (batch x num_nodes x num_nodes x hidden_dim)\n",
    "        \"\"\"\n",
    "        Ue = self.W_3(e)\n",
    "        Vx = self.W_4(x)\n",
    "        \n",
    "        # this enables us to make use of broadcasting to get a B x num_nodes x num_nodes x hidden_dim tensor\n",
    "        Vx_cols = Vx.unsqueeze(1) # B x num_nodes x hidden_dim => B x 1 x num_nodes x hidden_dim\n",
    "        Vx_rows = Vx.unsqueeze(2) # B x num_nodes x hidden_dim => B x num_nodes x 1 x hidden_dim\n",
    "        \n",
    "        e_new = Ue + Vx_rows + Vx_cols\n",
    "        \n",
    "        return e_new\n",
    "    \n",
    "class NodeFeatureLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    W_1 x_i + ( sum_j( n_ij * W_2 x_j ) )\n",
    "    \n",
    "    where: n_ij = gate_ij / sum_j ( gate_ij + e )\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.W_1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.epsilon = 1e-20\n",
    "        \n",
    "    def forward(self, x, edge_gate):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features (batch x num_nodes x hidden_dim)\n",
    "            edge_gate: Edge gate run through a sigmoid (batch x num_nodes x num_nodes x hidden_dim)\n",
    "        \"\"\"\n",
    "        W_1_x = self.W_1(x)\n",
    "        W_2_x = self.W_2(x) # B x num_nodes x hidden_dim\n",
    "        \n",
    "        n_ij = edge_gate / (self.epsilon + torch.sum(edge_gate, dim=2)) # B x num_nodes x num_nodes x hidden_dim\n",
    "        x_new = torch.sum(n_ij * W_2_x, dim=2) # B x num_nodes x hidden_dim\n",
    "        \n",
    "        return x_new\n",
    "\n",
    "# == GRAPH LAYER ==\n",
    "\n",
    "class GraphLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph layer for x_i and e_ij\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.node_feat = NodeFeatureLayer(hidden_dim)\n",
    "        self.node_norm = NodeNorm(hidden_dim)\n",
    "        self.edge_feat = EdgeFeatureLayer(hidden_dim)\n",
    "        self.edge_norm = EdgeNorm(hidden_dim)\n",
    "        \n",
    "    def forward(self, x, e):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features (batch x node_num x hidden_dim)\n",
    "            e: Edge features (batch x num_nodes x num_nodes x hidden_dim)\n",
    "        Return:\n",
    "            x: Aggrgated node features (batch x node_num x hidden_dim)\n",
    "            e: Aggragated edge features (batch x num_nodes x num_nodes x hidden_dim)\n",
    "        \"\"\"\n",
    "        # edges\n",
    "        e_feat = self.edge_feat(x, e)\n",
    "        \n",
    "        # edge gates\n",
    "        e_gates = F.sigmoid(e_feat)\n",
    "        \n",
    "        # nodes\n",
    "        x_feat = self.node_feat(x, e_gates)\n",
    "        \n",
    "        # normalisation\n",
    "        x_norm = self.node_norm(x_feat)\n",
    "        e_norm = self.edge_norm(e_feat)\n",
    "        \n",
    "        # activation\n",
    "        x_act = F.relu(x_norm)\n",
    "        e_act = F.relu(e_norm)\n",
    "        \n",
    "        # combine\n",
    "        x_new = x + x_act\n",
    "        e_new = e + e_act\n",
    "        \n",
    "        return x_new, e_new\n",
    "        \n",
    "# == MLP (Edge predictions) ==\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential()\n",
    "        \n",
    "        for i in range(hidden_layers - 1):\n",
    "            i += 1\n",
    "            self.layers.add_module(f'lin{i}', nn.Linear(in_dim, in_dim))\n",
    "            self.layers.add_module(f'relu{i}', nn.ReLU())\n",
    "            \n",
    "        self.layers.add_module('final', nn.Linear(in_dim, out_dim))\n",
    "        \n",
    "    def forward(self, e):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            e: Edge features (batch x num_nodes x num_nodes x hidden_dim)\n",
    "        Returns:\n",
    "            y: Edge predictions (batch x num_nodes x num_nodes x out_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.layers(e)\n",
    "\n",
    "    \n",
    "# == MAIN NETWORK ==\n",
    "\n",
    "class GraphNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # configs\n",
    "        self.hidden_dim = 8\n",
    "        self.num_gcn_layers = 2\n",
    "        self.num_mlp_layers = 2\n",
    "        \n",
    "        # embeddings\n",
    "        # TODO: Why is bias turned off when in the paper they don't mention anything?\n",
    "        self.node_coord_embedding = nn.Linear(2, self.hidden_dim, bias=False)\n",
    "        self.distance_embedding = nn.Linear(1, self.hidden_dim // 2, bias=False)\n",
    "        # TODO: Don't understand the use of the Embedding layer\n",
    "        self.edge_type_embedding = nn.Embedding(3, self.hidden_dim // 2) # 3 for the special cases 0, 1, 2 (more memory efficient)\n",
    "        \n",
    "        # GCN layers\n",
    "        self.gcn_layers = nn.ModuleList([\n",
    "            GraphLayer(hidden_dim=self.hidden_dim) for _ in range(self.num_gcn_layers)\n",
    "        ])\n",
    "        \n",
    "        # edge prediction MLP\n",
    "        self.mlp_edges = MLP(in_dim=self.hidden_dim, out_dim=2, hidden_layers=self.num_mlp_layers)\n",
    "        \n",
    "    def forward(self, node_coords, distance_matrix, edge_types):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            node_coords: Coordinates for each node (batch x num_nodes x num_nodes)\n",
    "            distance_matrix: Distance matrix between nodes (batch x num_nodes x num_nodes)\n",
    "            edge_types: Edge types to help learn faster (batch x num_nodes x num_nodes)\n",
    "        \"\"\"\n",
    "        # eq 2\n",
    "        x = self.node_coord_embedding(node_coords) # B x num_nodes x hidden_dim\n",
    "        \n",
    "        # eq 3\n",
    "        dist_unsqueezed = distance_matrix.unsqueeze(3) # B x num_nodes x num_nodes x 1\n",
    "        e_dist = self.distance_embedding(dist_unsqueezed) # B x num_nodes x num_nodes x hidden_dim // 2\n",
    "        e_types = self.edge_type_embedding(edge_types) # B x num_nodes x num_nodes x hidden_dim // 2\n",
    "        e = torch.cat((e_dist, e_types), dim=3) # B x num_nodes x num_nodes x hidden_dim\n",
    "        \n",
    "        # eq 4 and 5\n",
    "        for gcn_layer in self.gcn_layers:\n",
    "            _x, e = gcn_layer(x, e) # B x num_nodes x hidden_dim, B x num_nodes x num_nodes x hidden_dim\n",
    "            \n",
    "        # eq 6\n",
    "        y_edge_preds = self.mlp_edges(e) # B x num_nodes x num_nodes x 2\n",
    "        \n",
    "        return y_edge_preds\n",
    "\n",
    "model = GraphNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "be570bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.forward(batch_nodes_coord, batch_edges_values, batch_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "02beb4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 4, 2])"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec4cd05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
